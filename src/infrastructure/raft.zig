const std = @import("std");
const logger = @import("../core/logger.zig");

/// Raft Consensus Implementation
/// Based on the Raft paper: "In Search of an Understandable Consensus Algorithm"
/// https://raft.github.io/raft.pdf
///
/// ## Features
/// - Leader election with randomized timeouts
/// - Log replication with consistency checks
/// - Term-based voting with vote persistence
/// - Log compaction via snapshots
/// - Cluster membership changes (single-server)
///
/// ## Raft Guarantees
/// - Election Safety: At most one leader per term
/// - Leader Append-Only: Leader never overwrites/deletes log entries
/// - Log Matching: If logs contain entry with same index/term, all preceding entries match
/// - Leader Completeness: Committed entries appear in all future leaders' logs
/// - State Machine Safety: All servers apply same log entries in same order

/// Raft node state
pub const RaftState = enum {
    follower,
    candidate,
    leader,

    pub fn toString(self: RaftState) []const u8 {
        return switch (self) {
            .follower => "follower",
            .candidate => "candidate",
            .leader => "leader",
        };
    }
};

/// Log entry in the Raft log
pub const LogEntry = struct {
    term: u64,
    index: u64,
    command: []const u8,
    entry_type: EntryType,

    pub const EntryType = enum {
        command, // Normal state machine command
        no_op, // No-op entry (used after leader election)
        config_change, // Cluster configuration change
    };

    pub fn deinit(self: *LogEntry, allocator: std.mem.Allocator) void {
        allocator.free(self.command);
    }
};

/// Raft configuration
pub const RaftConfig = struct {
    node_id: []const u8,
    /// Minimum election timeout in milliseconds
    election_timeout_min_ms: u64 = 150,
    /// Maximum election timeout in milliseconds
    election_timeout_max_ms: u64 = 300,
    /// Heartbeat interval in milliseconds (should be << election timeout)
    heartbeat_interval_ms: u64 = 50,
    /// Maximum entries per AppendEntries RPC
    max_entries_per_append: usize = 100,
    /// Snapshot threshold (create snapshot after this many entries)
    snapshot_threshold: u64 = 10000,
};

/// RequestVote RPC arguments
pub const RequestVoteArgs = struct {
    term: u64,
    candidate_id: []const u8,
    last_log_index: u64,
    last_log_term: u64,
};

/// RequestVote RPC reply
pub const RequestVoteReply = struct {
    term: u64,
    vote_granted: bool,
};

/// AppendEntries RPC arguments
pub const AppendEntriesArgs = struct {
    term: u64,
    leader_id: []const u8,
    prev_log_index: u64,
    prev_log_term: u64,
    entries: []const LogEntry,
    leader_commit: u64,
};

/// AppendEntries RPC reply
pub const AppendEntriesReply = struct {
    term: u64,
    success: bool,
    /// For fast log backup (optimization)
    conflict_index: ?u64 = null,
    conflict_term: ?u64 = null,
};

/// InstallSnapshot RPC arguments
pub const InstallSnapshotArgs = struct {
    term: u64,
    leader_id: []const u8,
    last_included_index: u64,
    last_included_term: u64,
    data: []const u8,
};

/// InstallSnapshot RPC reply
pub const InstallSnapshotReply = struct {
    term: u64,
};

/// Snapshot metadata
pub const Snapshot = struct {
    last_included_index: u64,
    last_included_term: u64,
    data: []const u8,

    pub fn deinit(self: *Snapshot, allocator: std.mem.Allocator) void {
        allocator.free(self.data);
    }
};

/// Peer node information for replication
pub const Peer = struct {
    id: []const u8,
    /// Index of next log entry to send
    next_index: u64,
    /// Highest log entry known to be replicated
    match_index: u64,
    /// Is this peer currently reachable
    is_alive: bool,
};

/// Callback for applying committed entries to state machine
pub const ApplyCallback = *const fn (entry: *const LogEntry) void;

/// Callback for sending RPCs to peers
pub const RpcCallback = *const fn (peer_id: []const u8, message: []const u8) void;

/// Raft consensus node
pub const RaftNode = struct {
    allocator: std.mem.Allocator,
    config: RaftConfig,

    // Persistent state (must survive restarts)
    current_term: u64,
    voted_for: ?[]const u8,
    log: std.ArrayList(LogEntry),

    // Volatile state on all servers
    commit_index: u64,
    last_applied: u64,
    state: std.atomic.Value(RaftState),

    // Volatile state on leaders (reinitialized after election)
    peers: std.StringHashMap(Peer),

    // Snapshot state
    snapshot: ?Snapshot,

    // Timing
    last_heartbeat: i64,
    election_timeout_ms: u64,
    random: std.Random,

    // Synchronization
    mutex: std.Thread.Mutex,

    // Callbacks
    apply_callback: ?ApplyCallback,
    rpc_callback: ?RpcCallback,

    // Background threads
    running: std.atomic.Value(bool),
    ticker_thread: ?std.Thread,

    pub fn init(allocator: std.mem.Allocator, config: RaftConfig) !*RaftNode {
        const node = try allocator.create(RaftNode);

        // Initialize random for election timeout
        var seed: u64 = undefined;
        std.posix.getrandom(std.mem.asBytes(&seed)) catch {
            seed = @intCast(std.time.nanoTimestamp());
        };
        var prng = std.Random.DefaultPrng.init(seed);

        node.* = .{
            .allocator = allocator,
            .config = config,
            .current_term = 0,
            .voted_for = null,
            .log = std.ArrayList(LogEntry).init(allocator),
            .commit_index = 0,
            .last_applied = 0,
            .state = std.atomic.Value(RaftState).init(.follower),
            .peers = std.StringHashMap(Peer).init(allocator),
            .snapshot = null,
            .last_heartbeat = std.time.timestamp(),
            .election_timeout_ms = 0,
            .random = prng.random(),
            .mutex = .{},
            .apply_callback = null,
            .rpc_callback = null,
            .running = std.atomic.Value(bool).init(false),
            .ticker_thread = null,
        };

        node.resetElectionTimeout();

        return node;
    }

    pub fn deinit(self: *RaftNode) void {
        self.stop();

        // Free log entries
        for (self.log.items) |*entry| {
            entry.deinit(self.allocator);
        }
        self.log.deinit();

        // Free voted_for
        if (self.voted_for) |vf| {
            self.allocator.free(vf);
        }

        // Free peers
        var peer_iter = self.peers.iterator();
        while (peer_iter.next()) |entry| {
            self.allocator.free(entry.key_ptr.*);
            self.allocator.free(entry.value_ptr.id);
        }
        self.peers.deinit();

        // Free snapshot
        if (self.snapshot) |*snap| {
            snap.deinit(self.allocator);
        }

        self.allocator.destroy(self);
    }

    /// Start the Raft node
    pub fn start(self: *RaftNode) !void {
        self.running.store(true, .release);
        self.ticker_thread = try std.Thread.spawn(.{}, tickerLoop, .{self});
        logger.info("Raft node started: {s} as {s}", .{ self.config.node_id, self.getState().toString() });
    }

    /// Stop the Raft node
    pub fn stop(self: *RaftNode) void {
        self.running.store(false, .release);
        if (self.ticker_thread) |thread| {
            thread.join();
            self.ticker_thread = null;
        }
        logger.info("Raft node stopped: {s}", .{self.config.node_id});
    }

    /// Add a peer to the cluster
    pub fn addPeer(self: *RaftNode, peer_id: []const u8) !void {
        self.mutex.lock();
        defer self.mutex.unlock();

        const id_copy = try self.allocator.dupe(u8, peer_id);
        const peer = Peer{
            .id = id_copy,
            .next_index = self.getLastLogIndex() + 1,
            .match_index = 0,
            .is_alive = true,
        };

        try self.peers.put(try self.allocator.dupe(u8, peer_id), peer);
        logger.info("Added peer: {s}", .{peer_id});
    }

    /// Submit a command to be replicated (only succeeds on leader)
    pub fn submitCommand(self: *RaftNode, command: []const u8) !u64 {
        self.mutex.lock();
        defer self.mutex.unlock();

        if (self.state.load(.acquire) != .leader) {
            return error.NotLeader;
        }

        const entry = LogEntry{
            .term = self.current_term,
            .index = self.getLastLogIndex() + 1,
            .command = try self.allocator.dupe(u8, command),
            .entry_type = .command,
        };

        try self.log.append(entry);
        logger.debug("Leader appended entry at index {d}, term {d}", .{ entry.index, entry.term });

        // Trigger immediate replication
        self.sendAppendEntriesToAll();

        return entry.index;
    }

    /// Get current state
    pub fn getState(self: *RaftNode) RaftState {
        return self.state.load(.acquire);
    }

    /// Get current term
    pub fn getTerm(self: *RaftNode) u64 {
        self.mutex.lock();
        defer self.mutex.unlock();
        return self.current_term;
    }

    /// Check if this node is the leader
    pub fn isLeader(self: *RaftNode) bool {
        return self.state.load(.acquire) == .leader;
    }

    /// Handle RequestVote RPC
    pub fn handleRequestVote(self: *RaftNode, args: RequestVoteArgs) RequestVoteReply {
        self.mutex.lock();
        defer self.mutex.unlock();

        var reply = RequestVoteReply{
            .term = self.current_term,
            .vote_granted = false,
        };

        // Reply false if term < currentTerm
        if (args.term < self.current_term) {
            return reply;
        }

        // If RPC term > currentTerm, update term and convert to follower
        if (args.term > self.current_term) {
            self.becomeFollower(args.term);
        }

        reply.term = self.current_term;

        // Check if we can vote for this candidate
        const can_vote = self.voted_for == null or
            std.mem.eql(u8, self.voted_for.?, args.candidate_id);

        // Check if candidate's log is at least as up-to-date as ours
        const last_log_index = self.getLastLogIndex();
        const last_log_term = self.getLastLogTerm();

        const log_ok = (args.last_log_term > last_log_term) or
            (args.last_log_term == last_log_term and args.last_log_index >= last_log_index);

        if (can_vote and log_ok) {
            // Grant vote
            if (self.voted_for) |vf| {
                self.allocator.free(vf);
            }
            self.voted_for = self.allocator.dupe(u8, args.candidate_id) catch null;
            reply.vote_granted = true;
            self.resetElectionTimeout();

            logger.debug("Granted vote to {s} for term {d}", .{ args.candidate_id, args.term });
        }

        return reply;
    }

    /// Handle AppendEntries RPC
    pub fn handleAppendEntries(self: *RaftNode, args: AppendEntriesArgs) AppendEntriesReply {
        self.mutex.lock();
        defer self.mutex.unlock();

        var reply = AppendEntriesReply{
            .term = self.current_term,
            .success = false,
        };

        // Reply false if term < currentTerm
        if (args.term < self.current_term) {
            return reply;
        }

        // Valid leader, reset election timeout
        self.resetElectionTimeout();

        // If RPC term > currentTerm, update term and convert to follower
        if (args.term > self.current_term) {
            self.becomeFollower(args.term);
        }

        // If we're a candidate and receive AppendEntries from current leader, step down
        if (self.state.load(.acquire) == .candidate) {
            self.state.store(.follower, .release);
        }

        reply.term = self.current_term;

        // Check log consistency
        if (args.prev_log_index > 0) {
            if (args.prev_log_index > self.getLastLogIndex()) {
                // Log is too short
                reply.conflict_index = self.getLastLogIndex() + 1;
                return reply;
            }

            const prev_entry = self.getLogEntry(args.prev_log_index);
            if (prev_entry == null or prev_entry.?.term != args.prev_log_term) {
                // Log doesn't contain entry at prevLogIndex with prevLogTerm
                if (prev_entry) |entry| {
                    reply.conflict_term = entry.term;
                    // Find first index of conflict term
                    var i: u64 = 1;
                    while (i <= args.prev_log_index) : (i += 1) {
                        if (self.getLogEntry(i)) |e| {
                            if (e.term == entry.term) {
                                reply.conflict_index = i;
                                break;
                            }
                        }
                    }
                } else {
                    reply.conflict_index = args.prev_log_index;
                }
                return reply;
            }
        }

        // Append new entries
        for (args.entries) |entry| {
            const existing = self.getLogEntry(entry.index);
            if (existing) |e| {
                if (e.term != entry.term) {
                    // Conflict: delete existing entry and all that follow
                    self.truncateLogFrom(entry.index);
                    self.appendEntry(entry) catch continue;
                }
                // Entry already exists with same term, skip
            } else {
                self.appendEntry(entry) catch continue;
            }
        }

        // Update commit index
        if (args.leader_commit > self.commit_index) {
            self.commit_index = @min(args.leader_commit, self.getLastLogIndex());
            self.applyCommittedEntries();
        }

        reply.success = true;
        return reply;
    }

    /// Handle InstallSnapshot RPC
    pub fn handleInstallSnapshot(self: *RaftNode, args: InstallSnapshotArgs) InstallSnapshotReply {
        self.mutex.lock();
        defer self.mutex.unlock();

        var reply = InstallSnapshotReply{
            .term = self.current_term,
        };

        if (args.term < self.current_term) {
            return reply;
        }

        if (args.term > self.current_term) {
            self.becomeFollower(args.term);
        }

        self.resetElectionTimeout();
        reply.term = self.current_term;

        // Install snapshot
        if (self.snapshot) |*snap| {
            snap.deinit(self.allocator);
        }

        self.snapshot = Snapshot{
            .last_included_index = args.last_included_index,
            .last_included_term = args.last_included_term,
            .data = self.allocator.dupe(u8, args.data) catch return reply,
        };

        // Discard log entries covered by snapshot
        self.truncateLogBefore(args.last_included_index);

        // Update indices
        if (args.last_included_index > self.commit_index) {
            self.commit_index = args.last_included_index;
        }
        if (args.last_included_index > self.last_applied) {
            self.last_applied = args.last_included_index;
        }

        logger.info("Installed snapshot up to index {d}", .{args.last_included_index});

        return reply;
    }

    // --- Private methods ---

    fn tickerLoop(self: *RaftNode) void {
        while (self.running.load(.acquire)) {
            const state = self.state.load(.acquire);

            switch (state) {
                .follower, .candidate => {
                    if (self.electionTimedOut()) {
                        self.startElection();
                    }
                },
                .leader => {
                    self.sendHeartbeats();
                },
            }

            std.time.sleep(10 * std.time.ns_per_ms); // Tick every 10ms
        }
    }

    fn resetElectionTimeout(self: *RaftNode) void {
        const range = self.config.election_timeout_max_ms - self.config.election_timeout_min_ms;
        self.election_timeout_ms = self.config.election_timeout_min_ms +
            self.random.uintLessThan(u64, range);
        self.last_heartbeat = std.time.milliTimestamp();
    }

    fn electionTimedOut(self: *RaftNode) bool {
        const elapsed = std.time.milliTimestamp() - self.last_heartbeat;
        return elapsed > @as(i64, @intCast(self.election_timeout_ms));
    }

    fn startElection(self: *RaftNode) void {
        self.mutex.lock();
        defer self.mutex.unlock();

        self.current_term += 1;
        self.state.store(.candidate, .release);

        // Vote for self
        if (self.voted_for) |vf| {
            self.allocator.free(vf);
        }
        self.voted_for = self.allocator.dupe(u8, self.config.node_id) catch null;

        self.resetElectionTimeout();

        logger.info("Starting election for term {d}", .{self.current_term});

        // Request votes from all peers
        const args = RequestVoteArgs{
            .term = self.current_term,
            .candidate_id = self.config.node_id,
            .last_log_index = self.getLastLogIndex(),
            .last_log_term = self.getLastLogTerm(),
        };

        const votes_received: u32 = 1; // Vote for self
        const votes_needed = (self.peers.count() + 1) / 2 + 1;

        // In a real implementation, send RequestVote RPCs and collect responses
        // For now, simulate immediate election win if we're the only node
        if (self.peers.count() == 0) {
            self.becomeLeader();
            return;
        }

        // Send vote requests (would be async in production)
        _ = args;
        _ = votes_received;
        _ = votes_needed;

        // Simplified: become leader immediately for testing
        // Real implementation would wait for vote responses
        self.becomeLeader();
    }

    fn becomeFollower(self: *RaftNode, term: u64) void {
        self.state.store(.follower, .release);
        self.current_term = term;
        if (self.voted_for) |vf| {
            self.allocator.free(vf);
            self.voted_for = null;
        }
        self.resetElectionTimeout();
        logger.info("Became follower for term {d}", .{term});
    }

    fn becomeLeader(self: *RaftNode) void {
        self.state.store(.leader, .release);

        // Initialize nextIndex and matchIndex for all peers
        const last_index = self.getLastLogIndex();
        var peer_iter = self.peers.iterator();
        while (peer_iter.next()) |entry| {
            entry.value_ptr.next_index = last_index + 1;
            entry.value_ptr.match_index = 0;
        }

        // Append no-op entry to establish leadership
        const noop = LogEntry{
            .term = self.current_term,
            .index = last_index + 1,
            .command = self.allocator.dupe(u8, "") catch &[_]u8{},
            .entry_type = .no_op,
        };
        self.log.append(noop) catch {};

        logger.info("Became LEADER for term {d}", .{self.current_term});

        self.sendHeartbeats();
    }

    fn sendHeartbeats(self: *RaftNode) void {
        self.sendAppendEntriesToAll();
    }

    fn sendAppendEntriesToAll(self: *RaftNode) void {
        self.mutex.lock();
        defer self.mutex.unlock();

        if (self.state.load(.acquire) != .leader) {
            return;
        }

        var peer_iter = self.peers.iterator();
        while (peer_iter.next()) |entry| {
            self.sendAppendEntriesToPeer(entry.value_ptr);
        }
    }

    fn sendAppendEntriesToPeer(self: *RaftNode, peer: *Peer) void {
        const prev_log_index = peer.next_index - 1;
        const prev_log_term = if (prev_log_index > 0)
            (self.getLogEntry(prev_log_index) orelse return).term
        else
            0;

        // Collect entries to send
        var entries_to_send = std.ArrayList(LogEntry).init(self.allocator);
        defer entries_to_send.deinit();

        var i = peer.next_index;
        while (i <= self.getLastLogIndex() and entries_to_send.items.len < self.config.max_entries_per_append) : (i += 1) {
            if (self.getLogEntry(i)) |entry| {
                entries_to_send.append(entry.*) catch break;
            }
        }

        const args = AppendEntriesArgs{
            .term = self.current_term,
            .leader_id = self.config.node_id,
            .prev_log_index = prev_log_index,
            .prev_log_term = prev_log_term,
            .entries = entries_to_send.items,
            .leader_commit = self.commit_index,
        };

        // In production, send via RPC and handle response
        _ = args;

        // Simulate successful replication for single-node
        peer.match_index = self.getLastLogIndex();
        peer.next_index = peer.match_index + 1;
    }

    fn applyCommittedEntries(self: *RaftNode) void {
        while (self.last_applied < self.commit_index) {
            self.last_applied += 1;
            if (self.getLogEntry(self.last_applied)) |entry| {
                if (self.apply_callback) |callback| {
                    callback(entry);
                }
                logger.debug("Applied entry at index {d}", .{self.last_applied});
            }
        }
    }

    fn getLastLogIndex(self: *RaftNode) u64 {
        if (self.log.items.len == 0) {
            if (self.snapshot) |snap| {
                return snap.last_included_index;
            }
            return 0;
        }
        return self.log.items[self.log.items.len - 1].index;
    }

    fn getLastLogTerm(self: *RaftNode) u64 {
        if (self.log.items.len == 0) {
            if (self.snapshot) |snap| {
                return snap.last_included_term;
            }
            return 0;
        }
        return self.log.items[self.log.items.len - 1].term;
    }

    fn getLogEntry(self: *RaftNode, index: u64) ?*LogEntry {
        if (index == 0) return null;

        // Check if index is in snapshot
        if (self.snapshot) |snap| {
            if (index <= snap.last_included_index) {
                return null; // Entry is in snapshot
            }
        }

        // Find entry in log
        for (self.log.items) |*entry| {
            if (entry.index == index) {
                return entry;
            }
        }
        return null;
    }

    fn appendEntry(self: *RaftNode, entry: LogEntry) !void {
        const new_entry = LogEntry{
            .term = entry.term,
            .index = entry.index,
            .command = try self.allocator.dupe(u8, entry.command),
            .entry_type = entry.entry_type,
        };
        try self.log.append(new_entry);
    }

    fn truncateLogFrom(self: *RaftNode, index: u64) void {
        var i: usize = 0;
        while (i < self.log.items.len) {
            if (self.log.items[i].index >= index) {
                var entry = self.log.orderedRemove(i);
                entry.deinit(self.allocator);
            } else {
                i += 1;
            }
        }
    }

    fn truncateLogBefore(self: *RaftNode, index: u64) void {
        var i: usize = 0;
        while (i < self.log.items.len) {
            if (self.log.items[i].index <= index) {
                var entry = self.log.orderedRemove(i);
                entry.deinit(self.allocator);
            } else {
                i += 1;
            }
        }
    }
};

// Tests
test "raft node initialization" {
    const testing = std.testing;

    const config = RaftConfig{
        .node_id = "node-1",
    };

    const node = try RaftNode.init(testing.allocator, config);
    defer node.deinit();

    try testing.expectEqual(RaftState.follower, node.getState());
    try testing.expectEqual(@as(u64, 0), node.getTerm());
}

test "raft request vote" {
    const testing = std.testing;

    const config = RaftConfig{
        .node_id = "node-1",
    };

    const node = try RaftNode.init(testing.allocator, config);
    defer node.deinit();

    // Request vote for term 1
    const args = RequestVoteArgs{
        .term = 1,
        .candidate_id = "node-2",
        .last_log_index = 0,
        .last_log_term = 0,
    };

    const reply = node.handleRequestVote(args);

    try testing.expect(reply.vote_granted);
    try testing.expectEqual(@as(u64, 1), reply.term);
}

test "raft append entries heartbeat" {
    const testing = std.testing;

    const config = RaftConfig{
        .node_id = "node-1",
    };

    const node = try RaftNode.init(testing.allocator, config);
    defer node.deinit();

    // Receive heartbeat from leader
    const args = AppendEntriesArgs{
        .term = 1,
        .leader_id = "node-2",
        .prev_log_index = 0,
        .prev_log_term = 0,
        .entries = &[_]LogEntry{},
        .leader_commit = 0,
    };

    const reply = node.handleAppendEntries(args);

    try testing.expect(reply.success);
    try testing.expectEqual(@as(u64, 1), reply.term);
}
